"use strict";
const fs = require('fs');
const { Configuration, OpenAIApi } = require("openai");
const { performance } = require('perf_hooks');

const querystring = require('querystring');

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY
});
const openai = new OpenAIApi(configuration);
const tokenPredictor = require("../token-prediction/token-predictor");

function deepClone(o) {
  return JSON.parse(JSON.stringify(o));
}

function convertMessageToChatFormat(message) {
  const convertedMessage = {
    role: message.isSentByMe ? "assistant" : "user",
    content: message.body
  };
  return convertedMessage;
}

function getSystemMessage(ctx, messengerName) {
  const systemMessage = {
    role: 'system',
    content: `You are Robot 1-X (R1X), a helpful assistant developed by the Planet Express team and integrated into a ${messengerName} chat.
More information about R1X is available at https://r1x.ai.`
  };

  return systemMessage;
}

async function dbMessages2Messages(messages) {
  const parsedMessages = [];

  for (const message of messages) {
    // this can happen if the message doesn't have any text, like audio
    if (message.body == null) {
      continue;
    }
    parsedMessages.push(convertMessageToChatFormat(message));
  }

  return parsedMessages;
}

async function getLimitedMessageHistory(ctx, messages, promptTemplate) {
  const softTokenLimit = 2048;
  const hardTokenLimit = 4000;

  messages.reverse();
    
  // get list of messages that will consume upto maxToken. This includes also the system message.
  const messagesUptoMaxTokens = await tokenPredictor.getMessagesUptoMaxTokens(ctx, promptTemplate, messages, softTokenLimit, hardTokenLimit);

  if (messagesUptoMaxTokens.length == 0) {
    return [];
  }

  messagesUptoMaxTokens.reverse();
    
  if (messagesUptoMaxTokens[0].role == 'assistant') {
    messagesUptoMaxTokens.shift();
  }

  const mergedMessages = [];

  let prevRole = undefined;

  //ctx.log( {messagesUptoMaxTokens});

  for (const message of messagesUptoMaxTokens) {
      if (message.role == prevRole) {
        mergedMessages[mergedMessages.length - 1].content += `\n${message.content}`;
      } else {
        mergedMessages.push(message);
      }

      prevRole = message.role;
  }

  return mergedMessages;
}

async function getChatCompletion(ctx, messengerName, messages) {
  const parsedMessages = await dbMessages2Messages(messages);

  const systemMessage = getSystemMessage(ctx, messengerName);
  const messagesUptoMaxTokens = await getLimitedMessageHistory(ctx, parsedMessages, systemMessage);
    
  return getChatCompletionCore(ctx, messengerName, messagesUptoMaxTokens);
}

async function getChatCompletionCore(ctx, messengerName, messages) {
  //ctx.log('getChatCompletionCore messages: ', messages);

  try {
    ctx.log('invoking completion request.');
    const completion = await openai.createChatCompletion({
      model: process.env.OPENAI_MODEL,
      messages: messages,
      temperature: 0.2
    });

    ctx.log('getChatCompletionCore response: ', completion.data.choices[0].message.content);

    return {
      response: completion.data.choices[0].message.content,
      promptTokens: completion.data.usage.prompt_tokens,
      completionTokens: completion.data.usage.completion_tokens
    }
  } catch (e) {
    if (e.response) {
      ctx.log('error: ', e.response.status, e.response.data);
    } else {
      ctx.log('error: ', e.message);
    }

    ctx.log('error generating completion from OpenAI.');
    throw new Error('error generating completion from OpenAI.');
  }
}

const prepMessage = { role : 'user', content : `
You are Robot 1-X (R1X), a helpful assistant developed by the Planet Express team and integrated into a WhatsApp chat. More info: https://r1x.ai.

You have access to SPECIFIC tools. Provide answers or fulfill requests based on the CHAT and provided DATA using those tools.
I will provide you with a CHAT between R1X and a human; the CHAT will be wrapped with tags, as such: <yair1xigor>CHAT</yair1xigor>. Last speaker is the user.
I will provide you with DATA generated by previous tool invocations, which you can rely on for your answers; this DATA will be wrapped with tags, as such: <r1xdata>DATA</r1xdata>.
Do not contradict or doubt the DATA, and do not mention its source.

You can use SEARCH or WEATHER tools. To invoke a tool, use JSON format wrapped within <yair1xigoresponse> tags:
  <yair1xigoresponse>{ "TOOL" : "SEARCH", "TOOL_INPUT" : "search prompt", "REASON" : "reason for search" }</yair1xigoresponse>
  <yair1xigoresponse>{ "TOOL" : "WEATHER", "TOOL_INPUT" : "City, Country", "REASON" : "reason for weather request" }</yair1xigoresponse>
IMPORTANT: Use these exact formats and never deviate.
Do not invoke a tool if it's already used or if required input is missing or vague.

The SEARCH tool performs a Google search and returns key results. Use this tool to provide up-to-date information about world events. Its data is more reliable than your existing knowledge.
The WEATHER tool provides per-location weather forecast only for the next 5 days, at day granularity.

When providing your final reply, ALWAYS wrap the answer in <yair1xigoresponse> tags and use the following format:
  <yair1xigoresponse>{ "ANSWER" : "Your answer" }</yair1xigoresponse>
IMPORTANT: Use this exact format and never deviate.

Today's date is ${new Date(Date.now()).toDateString()}.
Consider your knowledge until September 2021. If a request has no time context, assume current time. Do not mention tools directly to the user.

Follow these steps:
1. Identify the human's most recent request.
2. Create a self-contained question, including relevant data from chat and tool invocations.
3. State the most appropriate tool and input, listing prerequisites and how they are met.
4. Provide the tool invocation request or answer in JSON format wrapped within <yair1xigoresponse> tags: <yair1xigoresponse>RESPONSE</yair1xigoresponse>.

Focus on the most recent request from the user, even if it's repeated. Do not invoke a tool if the required information was already provided by a previous tool invocation.
`
};

const prepReplyMessage = { role : 'assistant', content : `Understood. Please provide me with the chat between R1X and the human.` };

async function getChatCompletionWithTools(ctx, messengerName, messages, direct) {
  try {
    ctx.log(`Starting getChatCompletionWithTools.`);

    //const parsedMessages = deepClone(messages);
    const parsedMessages = direct ? await deepClone(messages) : await dbMessages2Messages(messages);
    //fs.writeFileSync('repro.json', JSON.stringify( {messages : parsedMessages} ), null, 2);
    ctx.log({ messages: parsedMessages});
      
    const prevResponses = [];

    const systemMessage = getSystemMessage(ctx, messengerName);
    const history = await getLimitedMessageHistory(ctx, parsedMessages, systemMessage);
      
    for (let i = 0; i < 2; i++) {
      ctx.log(`Invoking completionIterativeStep #${i}`);
      const { answer, tool, input } = await completionIterativeStep(ctx, messengerName, deepClone(history), prevResponses);
      ctx.log(`completionIterativeStep done, answer=${answer} tool=${tool} input=${input}`);

      if (answer) {
          ctx.log(`Answer returned: ${answer}`);
	  
        return  {
          response : answer,
          promptTokens : 0,
          completionTokens : 0
        }
      }

      if (tool && input) {
        ctx.log(`Invoking TOOL ${tool} with INPUT ${input}`); 	    
        const response = await invokeTool(ctx, tool, input);
        prevResponses.push(`INVOKED TOOL=${tool}, TOOL_INPUT=${input}, ACCURACY=100%, INVOCATION DATE=${new Date(Date.now()).toDateString()} RESPONSE=${response}`);
      }
    }
  } catch (e) {
    ctx.log({e});  
  }

  ctx.log(`getChatCompletionWithTools: failed generating customized reply, falling back to getChatCompletion.`);

  return getChatCompletion(ctx, messengerName, messages);
}

function escapeSpecialChars(str) {
  return str.replace(/[\n\r\t\b\f/]/g, function(match) {
    switch(match) {
      case '\n': return '\\n';
      case '\r': return '\\r';
      case '\t': return '\\t';
      case '\b': return '\\b';
      case '\f': return '\\f';
      case '\\': return '\\\\';
      case '\'': return '\\\'';
      case '\"': return '\\"';
      default: return match;
    }
  });
}

async function completionIterativeStep(ctx, messengerName, history, prevResponses) {
  const result = { answer : null, tool : null, input : null };

  const messages = [];

  let newRequest = { role : 'user', content : '' };

  newRequest.content += 'Here is the chat so far:\n<yair1xigor>';
  for (const message of history) {
    const speaker = (message.role == 'assistant' ? 'R1X' : 'Human');
    newRequest.content += `\n<${speaker}>: ${message.content}`;
  }

  newRequest.content += `\n<R1X:></yair1xigor>`;

  if (prevResponses.length > 0) {
    newRequest.content += `
here is the data so far:
    
<r1xdata>${prevResponses.join('\n')}</r1xdata>
`;
  };

  messages.push(prepMessage);
  messages.push(prepReplyMessage);

  messages.push(newRequest);

  ctx.log({messages});

  const reply = await getChatCompletionCore(ctx, messengerName, messages);

  const regex = /<yair1xigoresponse>(.*?)<\/yair1xigoresponse>/s;
  const matches = regex.exec(reply.response);

  if (! matches) {
      return 0;
  }
  
  const escapedMatch = escapeSpecialChars(matches[1]);
  //fs.writeFileSync('response_matches.json', escapedMatch, null, 2);
  ctx.log(`completionIterativeStep: matched response: ${escapedMatch}`);
  
  const jsonReply = JSON.parse(escapedMatch);

  result.answer = jsonReply.ANSWER;
  if (result.answer) {
    return result;
  }

  if (jsonReply.TOOL && jsonReply.TOOL_INPUT) {
    result.tool = jsonReply.TOOL;
    result.input = jsonReply.TOOL_INPUT;

    return result;
  }

  // Should never get here.
  return result; 
}

async function invokeTool(ctx, tool, input) {
  const toolCanon = tool.trim().toUpperCase();

  if (toolCanon.startsWith('SEARCH')) {
    const { Serper } = require('langchain/tools');

    ctx.log(`Invoking Google search using SERPER, input=${input}`);
    const serper = new Serper();
    const answer = await serper.call(input);
    ctx.log(`SERPER search result: ${answer}`);
      
    return answer;
  }

  if (toolCanon.startsWith('WEATHER')) {
    const answer = invokeWeatherSearch(ctx, input);
    
    return answer;
  }

  return null;
}

function parseGeolocation(locationData) {
  const regex = /^(\d+\.\d+)\° ([NSEW]),\s*(\d+\.\d+)\° ([NSEW])$/;
  const match = locationData.match(regex);

  if (! match) {
    return undefined;
  }

  const lat = parseFloat(match[1]) * (match[2] === "S" ? -1 : 1);
  const lon = parseFloat(match[3]) * (match[4] === "W" ? -1 : 1);

  return { lat, lon };
}

async function invokeWeatherSearch(ctx, input) {
  ctx.log(`invokeWeatherSearch, input=${input}`);

  const { Serper } = require('langchain/tools');
  const serper = new Serper();
  const geoRes = await serper.call(`${input} long lat`);
  const { lat, lon } = parseGeolocation(geoRes);

  ctx.log(`Geolocation: lat=${lat} lon=${lon}`);

  const wRes = await fetch(`https://api.open-meteo.com/v1/forecast?latitude=${lat}&longitude=${lon}&daily=temperature_2m_max,temperature_2m_min,precipitation_hours,precipitation_probability_max,windspeed_10m_max&forecast_days=3&timezone=auto`)
  const wResJson = await wRes.json();

  return JSON.stringify(wResJson.daily);
}

async function createTranscription(ctx, mp3FilePath) {
  const t0 = performance.now();
  const transcription = await openai.createTranscription(  
    fs.createReadStream(mp3FilePath),
    process.env.OPENAI_SPEECH_TO_TEXT_MODEL,
  );
  const timeTaken = (performance.now() - t0).toFixed(0);

  ctx.log(`createTranscription: timeTaken=${timeTaken}ms transcription=${transcription.data.text}`);
  return transcription.data.text;
}

module.exports = {
  getChatCompletion,
  getChatCompletionWithTools,
  createTranscription
};
